{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/thoyavan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/thoyavan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/thoyavan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# File paths\n",
    "file_path = \"arxiv-metadata-oai-snapshot.json\"\n",
    "output_file_path = \"cleaned_lemmatized_abstracts.txt\"\n",
    "\n",
    "# Setup\n",
    "stop_words = set(word.lower() for word in stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Data holders\n",
    "filtered_tokens_all = []\n",
    "list_of_abstract_token_lists = []\n",
    "abstract_count = 0\n",
    "max_abstracts = 10000\n",
    "\n",
    "# Open file to write cleaned abstracts\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if abstract_count >= max_abstracts:\n",
    "                break\n",
    "            try:\n",
    "                json_object = json.loads(line)\n",
    "                if \"abstract\" in json_object and json_object[\"abstract\"].strip():\n",
    "                    abstract = json_object[\"abstract\"]\n",
    "\n",
    "                    # Tokenize\n",
    "                    tokens = word_tokenize(abstract)\n",
    "\n",
    "                    # Clean and lemmatize\n",
    "                    filtered = [\n",
    "                        lemmatizer.lemmatize(word.lower(), pos='n')\n",
    "                        for word in tokens\n",
    "                        if word.isalpha() and word.lower() not in stop_words\n",
    "                    ]\n",
    "\n",
    "                    if filtered:\n",
    "                        # Add to data holders\n",
    "                        filtered_tokens_all.extend(filtered)\n",
    "                        list_of_abstract_token_lists.append(filtered)\n",
    "                        abstract_count += 1\n",
    "\n",
    "                        # Write to file (one line per abstract)\n",
    "                        output_file.write(\" \".join(filtered) + \"\\n\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "# Create DataFrame (summary of all tokens)\n",
    "df_filtered = pd.DataFrame({\n",
    "    'filtered_word_tokens': [filtered_tokens_all],\n",
    "    'combined_abstract_cleaned': [\" \".join(filtered_tokens_all)]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 50 most common words:\n",
      "model: 5064\n",
      "result: 3731\n",
      "field: 3464\n",
      "system: 3290\n",
      "show: 3213\n",
      "state: 2802\n",
      "also: 2708\n",
      "two: 2616\n",
      "energy: 2602\n",
      "using: 2438\n",
      "theory: 2424\n",
      "function: 2418\n",
      "study: 2399\n",
      "mass: 2331\n",
      "quantum: 2291\n",
      "one: 2136\n",
      "present: 2032\n",
      "data: 2017\n",
      "star: 1998\n",
      "equation: 1880\n",
      "case: 1856\n",
      "time: 1833\n",
      "method: 1811\n",
      "effect: 1776\n",
      "paper: 1769\n",
      "find: 1716\n",
      "density: 1710\n",
      "structure: 1709\n",
      "phase: 1690\n",
      "new: 1670\n",
      "space: 1656\n",
      "galaxy: 1656\n",
      "property: 1642\n",
      "parameter: 1640\n",
      "distribution: 1588\n",
      "number: 1557\n",
      "order: 1536\n",
      "solution: 1524\n",
      "magnetic: 1481\n",
      "large: 1444\n",
      "temperature: 1439\n",
      "spectrum: 1426\n",
      "problem: 1393\n",
      "different: 1387\n",
      "based: 1350\n",
      "observed: 1313\n",
      "group: 1303\n",
      "analysis: 1286\n",
      "first: 1282\n",
      "term: 1257\n"
     ]
    }
   ],
   "source": [
    "# Uses the new file created which contains the cleaned and lemmatized abstracts\n",
    "cleaned_file_path = \"cleaned_lemmatized_abstracts.txt\"\n",
    "\n",
    "word_freq = Counter()\n",
    "\n",
    "with open(cleaned_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        tokens = line.strip().split()\n",
    "        word_freq.update(tokens)\n",
    "\n",
    "# Shows the most common words\n",
    "print(\"\\nTop 50 most common words:\")\n",
    "for word, count in word_freq.most_common(50):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
